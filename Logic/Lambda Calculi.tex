\documentclass[12pt]{article}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}

\usepackage{import}
\import{./}{LogicCommands}

\newcommand{\red}{\triangleright}
\newcommand{\aconv}{\equiv_{\alpha}}
\newcommand{\bredo}{\red_{\nu,1}}
\newcommand{\bred}{\red_{\nu}}
\newcommand{\etared}{\red_{\eta}}
\newcommand{\etaredo}{\red_{\eta,1}}
\newcommand{\bered}{\red_{\nu\eta}}
\newcommand{\beredo}{\red_{\nu\eta,1}}


\newcommand{\br}[1]{\llbracket #1 \rrbracket}


\begin{document}
\author{Benjamin Church}
\title{\Huge $\lambda$-Calculi}

\section{Introduction}


The $\lambda$-calculus (here we first introduce pure untyped $\lambda$-calculus) is a formal system designed to capture of the notions of ``function'' and ``composition''.

\begin{rmk}
A $\lambda$-expression will be a finite string made from the symbols $\lambda$, $.$, and an infinite list of variable symbols $x, y, z, \cdots$ or $a,b,c, \cdots$ or $x_1, x_2, x_3, \dots$ whatever you want to call them. Think of $\lambda x . M$ as the function taking in $x$ and returing $M$ where $M$ is an expression possibly involving $x$. 
\end{rmk} 

\begin{defn}
A \textit{well-formed} $\lambda$-expression, or $\lambda$-\textit{term}, is defined recursively via,
\begin{enumerate}
\item any variable is a $\lambda$-term
\item if $M$ is a $\lambda$-term then $\lambda x . M$ is a $\lambda$-term
\item if $M$ and $N$ are $\lambda$-terms then $(M \, N)$ is a $\lambda$-term.
\end{enumerate}
The set of $\lambda$-terms is denoted $\Lambda$.
\end{defn}

\begin{rmk}
We have already said that we should interpret $\lambda x . M$ as a function. Then $(M, N)$ is an ``application'' of the function $M$ to $N$. We think of $((\lambda x . M) N)$ ``evaluating'' to $M[x := N]$ which is how this system captures the essence of functions and computation as evaluation though substitution. Although intutively we think of $\lambda$-terms as functions, that actually take an input and produce a well-defined output, this is actually difficult to define because we will have to decide when a computation is ``finished'' and in fact computations may not halt complicating our desire to call these things functions. A major goal will be to somehow interpret these objects as honest-to-god functions. For now, we take a different perspective not that $\lambda$-terms are ``machines'' but rather they are formal strings in a formal system. To create a formal system we need ``rules of inference'' which are conventionally called conversions and reductions.
\end{rmk}

\begin{defn}
A $\lambda$-term of the form $(\lambda x.M) \, N$ is called a $\nu$-\textit{redex} and its $\nu$-\textit{reduction} is the corresponding term $M [x := N]$. We write,
\[ (\lambda x.M) \, N \bredo M [x := N] \]
If a term $M$ can be converted to $N$ via a finite sequence of $\nu$-reductions we write,
\[ M \bred N \]
Then the equivalence relation generated by this (using zigzags) is called $\nu$-equivalence and writen as $M =_\nu N$. 
\end{defn}

\begin{example}
$( \lambda x . x) y \bredo y$ so we say that $\lambda x . x$ is the identity function.
\end{example}

\begin{defn}
A $\lambda$-term $M$ is a \textit{normal form} if it does not contain any $\nu$-redices. We say that $N$ \textit{is a normal form} of $M$ if $N$ is a normal form and $M \bred N$.
\end{defn} 

\begin{theorem}[Church-Rosser]
Let $M, P, Q$ be $\lambda$-terms such that $M \bred P$ and $M \bred Q$ then there exists a $\lambda$-term $T$ such that $P \bred T$ and $Q \bred T$. We express this with the diagram,
\begin{center}
\begin{tikzcd}[row sep = large]
& M \arrow[rd, "\bred"] \arrow[ld, "\bred"'] 
\\
P \arrow[rd, dashed, "\bred"'] & & Q \arrow[ld, dashed, "\bred"]
\\
& T
\end{tikzcd}
\end{center}
\end{theorem} 

\begin{cor}
Suppose $M, N$ are $\lambda$-terms with $M =_\nu N$ then there exists a $\lambda$-term $T$ such that $M \bred T$ and $N \bred T$. We say that $M$ and $N$ ``compute the same value''.
\end{cor}

\begin{cor}
The normal form of $M$ (if it exists) is unique.
\end{cor}

\begin{example}
A $\lambda$-term need not admit a normal form. For example let,
\[ \omega \equiv \lambda x . (x x) \]
this is the function that applies its input to itself. What happens if we apply $\omega$ to itsef then we get,
\[ \Omega \equiv (\omega \omega) \]
and it is easy to see that,
\[ \Omega \bredo \Omega \]
and there are no other possible $\nu$-reductions. Therefore $\Omega$ does not have a $\nu$-normal form. We can think of it as corresponding to a computation that does not halt.
\end{example}

\section{Arithmetic and Logic in the $\lambda$-Calculus}

Functions have a built-in way of expressing natural numbers, namely counting the number of iterates of a function. Therefore we can define the Church numerals as follows,
\[ \underline{n} \equiv \lambda f . \lambda x . (f^n \; x) \]
where $f^n$ means the expression $(f \; (f \; (f \; \cdots)))$ iterated $n$ times. We think of $n$ as expressing the function which takes in a function $f$ and returns its $n^{\text{th}}$ iterate. We can use this to define sucessor,
\[ \text{succ} \equiv \lambda n . (\lambda f . \lambda x. f \, (n \; f \; x)) \]
and addition,
\[ \text{plus} \equiv \lambda m . \lambda n. \lambda f. \lambda x . m \; f \; (n \; f \; x) \equiv \lambda m . \lambda n . \lambda f . (m \; \text{succ}) \; n\]
which is just composition of the two iterations or equivalently $(m \; \text{succ})$ which is the function that applies the sucessor function $m$ times applied to $n$. However, it is multiplication which shows the true flexibility of the system. We define,
\[ \text{mult} \equiv \lambda m . \lambda n . \lambda f . \lambda x . (m \; (n \; f) \; x) \equiv \lambda m . \lambda n . (m \; (\text{plus} \; n)) \]
which applies $m$ the ``apply its input $m$ times'' function to $(n \; f)$ which is the function $f^n$ so we get $mn$ iterates of $f$. Equivalently we can iterate $(\text{plus} \; n)$ the ``add $n$'' function $m$ times. 
Likewise, boolean values are naturally represented by their corresponding branching behavior, true should take in two functions and return the first while false should take in two functions and do the latter such that applying a boolean executes an if-then statment. Thus,
\[ \text{true} \equiv \lambda p . \lambda q . p \quad \text{and} \quad \text{false} \equiv \lambda p . \lambda q . q \]
I will leave it as an exercise to produce $\lambda$-terms expressing the basic logical operations. 

\begin{defn}
We say that a function $f : \N \to \N$ is \textit{represented} by an untyped $\lambda$-term $F$ if for all $n \in \N$,
\[ (F \; \underline{n}) =_{\nu} [\underline{f(n)}] \]
If there exists such an $F$ we say that $f$ is representable. If $F$ can be chosen such that it has a normal form then we say that $f$ is strongly representable.
\end{defn}

\begin{rmk}
We will see that every computable function is representable but not necessarily strongly representable. 
\end{rmk}

\section{``Consistency'' of the $\lambda$-Calculus (SKIP)}

EARLY FORM INCONSISTENT

Look at section here https://plato.stanford.edu/entries/lambda-calculus/

and in Barendregt's book. 

THIS FORM IS CONSISTENT IN THE SENSE OF CANNOT DERIVE EVERYTHING

CURY'S PARADOX

\section{Recursion and Fixed Points}

We've discussed how $\lambda$-terms can represent integer functions. However, so far we've only implemented very simple function such as addition and multiplication. It is not at all clear how to implement complex behavior without loops. In fact, we want to be able to implement general recursion but this seems impossible without self-referential definitions which are not allowed in the construction of our formal language. 
\bigskip\\
Although there does not appear to be a way to \textit{construct} recursive functions we do have a way to check if a given $\lambda$-term represents the required recursive function using a fixed-point expression. Indeed, suppose that we want to find a function $f$ which satisfies the recursive definiton,
\[ f(x) = 
\begin{cases}
g(f(x-1), x) & x > 0
\\
n & x = 0
\end{cases}\]
Suppose moreover that $g$ is represented by a $\lambda$-term $G$. Then consider the $\lambda$-term,
\[ T :\equiv \lambda f . \lambda x . (\text{IsZero} \; x) \; \underline{n} \; (G \; (f \; (\text{pred} \; x)) \; x)) \]
which takes a function $f$ and returns the function expressed by the RHS of the recursive definition. Therefore, we are searching for a $\lambda$-term $F$ such that,
\[ (T \; F) =_\nu F \]
This will then represent the function $f$. Therefore, we have reduced the problem to constructing fixed-points of $T$ up to $\equiv_{\nu\eta}$.  
\bigskip\\
Consider the following $\lambda$-term,
\[ Y :\equiv \lambda f. (\lambda x. f \, (x x)) \; (\lambda x. f \, (x x)) \]
called Haskell Curry's paradoxical combinator. Since that is quite a mouthful and its usual name has been coopted, I will simply call it ``Y''. 
Notice that, like $\Omega$, we have,
\[ (Y \; f) \bredo (f \; (Y \; f)) \]
In particular, $(Y \; T)$ is a fixed point of $T$. Therefore, astonishingly, in untyped $\lambda$-calculus, every $\lambda$-term admits a fixed point. Hence the recursion problems allways admit solutions. 

PHILOSOPHICAL MUSINGS ON THE NATURE OF THESE FIXED POINT PROBLEMS


ANOTHER WAY TO GET FIXED POINTS, ARE THESE THE SAME??

To even make sense of this question, we first need a notion of convergence of functions suitable for these integer functions. Moreover, because we want to ask if the limit of the representing $\lambda$-terms agree, we need some sort of \textit{model} in which the $\lambda$-terms are \textit{all} representing functions which we can reason about topologically. This is the subject we now turn to. 

\section{$\lambda$-Models (DO AT THE END)}

From the begining, the question arrose: if $\lambda$-terms are supposed to be ``anonomous functions'' what space are they functions on? More precisely, we want a notion of a space such that it's automorphisms are exactly the equivalence classes of $\lambda$-terms. This space must have some curious properties. Since $\lambda$-terms can be applied to any other $\lambda$-term it must be the case that each point of the space represents a function on it and every ``nice'' function arises in this fasion. For example, this couldn't be done in the category of sets since it would be saying there is a set $X$ with $2^X = X$ which is impossible by Cantor's theorem. We need a more interesting notion to capture what's going on in the untyped $\lambda$-calculus. 

\newcommand{\Var}{\mathrm{Var}}

\begin{defn}
A $\lambda$-\textit{interpretation} is a set $D$ and a \textit{valuation} function $\rho : \Var \to D$
\end{defn}

\begin{defn}
A $\lambda$-\textit{model} is a triple $(D, \bullet, \dbrac{-})$ with $\bullet$ a binary operation on $D$ and $\dbrac{-}$ a function giving for each valuation $\rho$ a mapping $\dbrac{-}_\rho : \Lambda \to D$ which satisfies,
\begin{enumerate}
\item if $x$ is a variable $\dbrac{x}_\rho = \rho(x)$
\item for any $\lambda$-terms $M, N$ then $\dbrac{M \, N}_\rho = \dbrac{M}_{\rho} \bullet \dbrac{N}_\rho$
\item for all variables $x$, terms $M$, and elements $d \in D$ we have $\dbrac{\lambda x . M}_\rho \bullet d = \dbrac{M}_{\rho [x := d]}$
\item for all terms $M$ and valuations $\rho, \sigma$ we have $\dbrac{M}_\rho = \dbrac{M}_\sigma$ if $\rho(x) = \sigma(x)$ for all free varaibles $x$ of $M$
\item for all terms $M$ and variables $x,y$ then $\dbrac{\lambda x . M}_\rho = \dbrac{\lambda y . M [x := x]}_{\rho}$ if $y$ is not free in $M$
\item for all terms $M, N$ if for all $d \in D$ we have $\dbrac{M}_{\rho [x := d]} = \dbrac{N}_{\rho [x := d]}$ then $\dbrac{\lambda x . M}_\rho = \dbrac{\lambda x . N}_\rho$.
\end{enumerate}
\end{defn}

\begin{rmk}
The \textit{term model} or \textit{trivial model} is given by $\Lambda$ modulo $\nu$-equivalence and composition given by application. This is a model but it is unenlightening. Dana Scott searched for a more interesting model. 
\end{rmk}

\subsection{Syntax-Free Models}

DO THIS

\section{Domain Theory}

\begin{rmk}
We use ``monotone'' and ``order-preserving'' synonymously to mean
\[ x \le y \implies f(x) \le f(y) \]
\end{rmk}


\begin{defn}
A \textit{directed-complete partial order} (dcpo) is a poset $(P, \le)$ such that every directed subset has a supremum. 
\end{defn}

\begin{rmk}
A directed subset $D \subset P$ is a subset such that every finite subset of $D$ has an upper bound in $D$. Equivalently for each $a, b \in D$ there is $c \in D$ with $a,b \le c$. 
\end{rmk}

\begin{rmk}
A somewhat technical axiom of choice argument shows that it suffices to check that every chain has a supremum in order for a poset to be a dcpo. 
\end{rmk}

\begin{rmk}
It is good to contrast a dcpo with the related notion of the complete lattice, a poset such that every subset has a supremum. Taking the supremum over the set of lower bounds gives all infima as well. Then a complete lattice is a lattice because the supremum and infimum of $\{ a, b \}$ give the meets and joins. However, $\{ a, b \}$ is \textit{not} directed and therefore we should not expect a dcpo to be either a meet or a join semilattice.  
\end{rmk}

\begin{defn}
A \textit{pointed dcpo} or \textit{cpo} is a dcpo $(P, \le)$ with a least element $\bot \in P$.
\end{defn}

\begin{rmk}
Remember what we are after, spaces that are naturally $\lambda$-modules. Also we are expecting that, using $Y$, any function arising from a $\lambda$-term has a fixed point and therefore we are looking for spaces whose nice functions all have fixed points. The following proposition shows that we are on the right track. 
\end{rmk}

\begin{prop}
Let $P$ be a pointed poset. Then the following are equivalent,
\begin{enumerate}
\item $P$ is a cpo
\item every monotone map $f : P \to P$ has a least fixpoint.  
\end{enumerate}
\end{prop}

\subsection{Continuity}

\begin{defn}
Let $f : P \to Q$ be a monotone map of dcpos. We say that $f$ is,
\begin{enumerate}
\item \textit{Scott-continuous} if for every directed subset $D \subset P$ we have $f(\sup D) = \sup f(D)$
\item \textit{strict} if $P$ and $Q$ are pointed and $f(\bot) = \bot$.
\end{enumerate}
We denote the poset (pointwise) of continuous functions by $[P \to Q]$ and the subposet of strict continuous functions by $[P \to Q]_{\bot}$.
\end{defn}

\begin{prop}
If $P$ and $Q$ are (pointed) dcpos then $[P \to Q]$ ($[P \to Q]_{\bot}$) is a (pointed) dcpos where suprema are computed pointwise. 
\end{prop}

\begin{proof}
Let $D \subset [P \to Q]$ be directed. By definition $A_x = \{ f(x) \mid f \in D \}$ is directed so the function,
\[ g(x) = \sup A_x = \sup_{f \in D} f(x) \]
is well-defined. Then for any directed $A \subset P$ notice that,
\[ g(\sup A) = \sup_{f \in D} f(\sup A) = \sup_{f \in D} \sup_{x \in A} f(x) = \sup_{x \in A} \sup_{f \in D} f(x) = \sup_{x \in A} g(x) \]
proving that $g$ is continuous. It is clear that $g$ is the supremum of $D$. 
\end{proof}

\subsection{The Scott Topology}

\begin{defn}
Let $P$ be a dcpos a subset $C \subset P$ is \textit{(Scott) closed} if it is downward and closed under suprema of directed subsets. The \textit{Scott topology} has these as closed sets.  
\end{defn}

\begin{prop}
A map $f : P \to Q$ of dcpos is Scott-continuous if and only if it is continuous in the Scott topology.  
\end{prop}

\begin{proof}
Assume $f$ is Scott-continuous and $C \subset Q$ is closed. Then $f^{-1}(C)$ is downward because $f$ is monotone and if $D \subset f^{-1}(C)$ is directed then $f(\sup D) = \sup f(D) \in C$ so $\sup D \in f^{-1}(C)$. 
\bigskip\\
Conversely, if $f$ is continuous for the Scott topology. For any $x \in P$ the set $D_x = \{ y \in P \mid y \le x \}$ is closed and if $x' \le x$ then $x \in f^{-1}(D_{f(x)}$ so $x' \in f^{-1}(D_{f(x)})$ since it is closed thus $f(x') \in D_{f(x)}$ so $f(x') \le f(x)$ so $f$ is monotone. Furthermore, let $D \subset P$ be directed. Then $f^{-1}(D_{\sup f(D)})$ is closed and $D \subset f^{-1}(D_{\sup f(D)})$ so $\sup D \in f^{-1}(D_{\sup f(D)})$ so $f(\sup D) \le \sup f(D)$ but also $f(D) \le f(\sup D)$ so $f(\sup D) = \sup f(D)$.  
\end{proof}

\subsection{Fixed Points}

\renewcommand{\fix}{\mathsf{fix}}
\renewcommand{\it}{\mathsf{it}}

When you first learn about functions and fixpoints you might notice that sometimes to find a fixpoint you can just iterate $f$. Take an arbitrary $x_0$ and then consider the sequence $f(x_0), f^2(x_0),  f^3(x_0), \dots$. If this converges it is guaranteed to be a fixpoint. For example, when I was bored in highschool, I took a random number on my calculator and hit $\cos$ over and over. It allways approaches $0.739085 \cdots$ the cosine fixpoint. It turns out, for continuous $f$, this process always converges in a cpo. 

\begin{theorem}[Kleene]
Let $D$ be a cpo,
\begin{enumerate}
\item every continuous $f : D \to D$ has a least fixpoint,
\[ \fix(f) = \sup_{n \in \N} f^n(\bot) \]
\item the map $\fix : [D \to D] \to D$ is continuous. 
\end{enumerate}
\end{theorem}

\begin{proof}
The set $\{ f^n(\bot) \}_{n \in \N}$ is a chain by monotonicity. Thus by completeness,
\[ f(\sup_{n \in \N} f^n(\bot)) = \sup_{n \in \N} f(f^n(\bot)) = \sup_{n \in \N} f^{n+1}(\bot) = \sup_{n \in \N} f^n(\bot) \]
so $\fix(f)$ is a fixpoint of $f$. Let $x \in D$ be any other fixpoint. By monotonicity $f^n(\bot) \le f^n(x) = x$ and thus $\sup_{n \in \N} f^n(\bot) \le x$.
\bigskip\\
Now we address the continuity of $\fix$. Notice that $\fix = \sup_{n \in \N} \it_n$ where $\it_n : f \mapsto f^n(\bot)$. Since $[[D \to D] \to D]$ is a dcpo it suffices to show that $\it_n$ is continuous. We proceed by induction. $\it_0$ is constant so this is continuous. Then for $F \subset [D \to D]$ directed, consider,
\begin{align*}
\it_{n+1}(\sup F) & = (\sup F)(\it_n(\sup F)) = (\sup F)(\sup_{f \in F} \it_n(f)) 
\\
& = \sup_{g \in F} g (\sup_{f \in F}(\it_n(f))) = \sup_{g \in F} \sup_{F \in F} g(\it_n(f)) = \sup_{f \in F} \sup_{g \in F} g(\it_n(f))
\\
& = \sup_{f \in F} f(\it_n(f)) 
\end{align*}
because the map $(f, g) \mapsto g(\it_n(f))$ is monotone. 
\end{proof}

\subsection{Scott Domains}

\begin{defn}
Let $P$ be a partially ordered set. Then $x \in P$ is a \textit{compact element} if for every directed subset $D \subset P$ with $x \le \sup{D}$ then $x \le d$ for some $d \in D$.
\end{defn}

\begin{rmk}
If we think of the partial order $P$ as a category then $\sup$ of a directed subset is exactly the filtered colimit. Thus compact elements of $P$ are exactly the compact objects of this category.
\end{rmk}

\begin{defn}
A nonemtpy dcpo $D$ is a \textit{Scott domain} if $D$ is 
\begin{enumerate}
\item \textit{bounded-complete}, i.e. all bounded subsets of $D$ have a supremum
\item \textit{algebraic} or \textit{compactly-generated}, i.e. every element of $D$ is obtained as the supremum of a directed set of compact elements of $D$.
\end{enumerate}
\end{defn}

\subsection{Scott's Model $D_{\infty}$}

To build a $\lambda$-model, we start with the trivial cpo $\N^+$ which is $\N$ with the trivial order adjoin $\bot$. We think of these elements as the Church numerals along with a symbol for ``ill-defined'' or ``DNE''. Then we get an interpretation of $\lambda$-terms as follows. If $M =_\nu \underline{n}$ then we send $M \mapsto n$ and otherwise $M \mapsto \bot$. This is well-defined because Church numerals are normal so no two can be $\nu$-equivalence by the Church-Roser theorem. However, this alone is not a $\lambda$-model, there is no composition law! What we need to do, is glue in the continuous functions $[\N^+ \to \N^+]$ so we can apply them to our elements $\N^+$. We then interpret a $\lambda$-term as $M \mapsto f$ for $f \in [\N^+ \to \N^+]$ such that for each $n \in \N$ we have $(M \; \underline{n}) =_\nu \underline{f(n)}$ and $f(n) = \bot$ if $(M \; \underline{n})$ does not reduce to a Church numeral. Finally, I need to say what $f(\bot)$ is. To impose continuity $f(\bot) = \bot$ unless $f|_{\N}$ is constant and then $f(\bot) = f(\N)$. Howver, we have not completed a $\lambda$-model because we don't know how to apply integer functions to eachother or how to actually do this ``gluing''. The ``building-up'' process of cpos is accomplished by the following construction,

\begin{defn}
Let $D, D'$ be cpos. A \textit{projection} from $D' \to D$ is a pair $(\phi, \psi)$ of continuous maps $\phi : D \to D'$ and $\psi : D' \to D$ such that,
\[ \psi \circ \phi = \id_D \quad \text{and} \quad \phi \circ \psi \le \id_{D'} \]
\end{defn}


Now we construct a sequence of cpos inductively. Let $D_0 = \N^+$ (adjoin $\bot$ to $\N$ equipped with the trivial order) then define $D_{n+1} = [D_n \to D_n]$. We construct projections $D_{n+1} \to D_n$ inductively as follows. Let $\phi_0(d) = \kappa_d$ where $\kappa$ is the constant function and $\psi_0(c) = c(\top)$. Then we define $\phi_n : D_n \to D_{n+1}$ and $\psi_n : D_{n+1} \to D_n$,
\[ \phi_n(\sigma) = \phi_{n-1} \circ \sigma \circ \psi_{n-1} \quad \text{ and } \quad \psi_n(\tau) = \psi_{n-1} \circ \tau \circ \phi_{n-1} \]
It is not difficult to show that this gives a sequence of projections. Then we define Scott's model,
\[ D_{\infty} = \varinjlim_{n} D_n \]
Explicitly the elements are sequences $(d_0, d_1, d_2, \dots)$ such that $\psi_n(d_{n+1}) = d_{n}$ for all $n$ with the ordering pointwise. 

\begin{prop}
$D_\infty$ is a cpo with $\bot = (\bot_0, \bot_1, \bot_2, \dots)$ and if $X \subset D_{\infty}$ is directed then,
\[ \sup X = (\sup X_0, \sup X_1, \sup X_2, \dots) \]
and there are projections $D_{\infty} \to D_n$ with $\phi_n : d \mapsto d_n$ and corresponding inclusion $\psi_n : d \mapsto (\psi_{n,0}(d), \psi_{n,1}(d), \psi_{n,2}(d), \dots)$.
\end{prop}

We can then define the composition on $D_{\infty}$ as,
\[ a \bullet b = \sup_n \psi_n(a_{n+1}(b_n)) \]

\begin{prop}
The composition gives an isomorphism $D_{\infty} \iso [D_{\infty} \to D_{\infty}]$. 
\end{prop}

Then we can show there is a natural way to make $(D_\infty, \bullet)$ into a $\lambda$-model extending the interpretation we gave for the first two stages. How are we supposed to think about elements of $D_\infty$. The relation $\le$ is expressing the idea of ``defined on a larger set''. Indeed, for $f,g \in D_1$ we have $f \le g$ iff $f(n) = g(n)$ if $f(n) \neq \bot$ and whenever $g(n) = \bot$ then $f(n) = \bot$ meaning $g$ is defined at least everywhere $f$ is and on the domain where $f$ is defined the two functions agree. Then the global bottom element $\bot \in D_\infty$ represents the function defined nowhere. Thefore, our supremum processes can be thought of as producing the limit of a sequence of functions that are getting progressively defined more often. 

\subsection{$D_\infty$ is a $\lambda$-model}

THINK OF THIS AS COMPUTABLE APPROXIMATION

\subsection{Fixpoints in $D_{\infty}$}

Using our interpretation of $D_{\infty}$ recall that the fixpoint operator takes the form,
\[ \fix(f) = \sup_{n \in \N} f^n(\bot) \]
Consider the recursion operator $T$ we defined before. Remember that $\fix(T)$ is a solution to the self-referential recursive definition. Explicitly, we get a sequence of functions,
\[ \bot, T(\bot), T^2(\bot), T^3(\bot), \dots \]
What do these mean? The first is just defined nowhere. The second is the function (when applied to Church numerals),
\[ T(\bot)(x) = \begin{cases}
n & x = 0
\\
\bot & x > 0
\end{cases} \]
so it is defined just at $x = 0$ to equal the base case. Then,
\[  T^2(\bot)(x) = 
\begin{cases}
n & x = 0
\\
G(n, 1) & x = 1
\\
\bot & x > 1
\end{cases}
\]
and so on. We see that this is converging to the expected behavior of the recursive function. 
\bigskip\\
However, recall that we have a different mysterious way of producing fixed points, the operator $Y$. In Scott's model $Y = \fix$ but this does not always happen.

\begin{rmk}
Using the $D_{\infty} \iso [D_{\infty} \to D_{\infty}]$ is an isomorphism we can regard $\fix \in D_{\infty}$. 
\end{rmk}

\begin{prop}
In $D_{\infty}$ we have $\br{Y} = \fix$.
\end{prop}

\begin{proof}
It is clear that $\fix \le Y$ since $\fix$ gives the smallest fixpoint. Conversely, for $x \in D_{\infty}$ we have,
\[ Y \bullet x = X \bullet X \]
where $X$ is such that $X \bullet d = x \bullet (y \bullet y)$. Then by definition,
\[ X \bullet X = \sup_{n \in \N} \psi_n(X_{n+1}(X_n)) \]
{\color{red} FINISH THIS}
Look here \chref{https://wrap.warwick.ac.uk/46310/}{The Y-combinator in Scott's lambda-calculus models}.
\end{proof}

\section{The Simply-Typed $\lambda$-Calculus}

Untyped $\lambda$-calculus has a simplictic beauty and is maximally powerful. However, this power and flexibility comes at a cost as Church originally found when is logical system built from the untyped calculus was found to contain contradictions. 
\bigskip\\
Adding types to the language is advantangeous from two perspectives. From the perspectives of logic, it allows the creation of a well-founded and consistent system that avoids paradoxes by building up terms hierachically (hence excising self-references in the way that achieved by Russel and Whitehead in \textit{Principia Mathematica}). From the perspective of programing language theory, types constrain functions \textit{formally} and therefore aid in the design and interpretation of programs, languages, and compilers. Futhermore, it is usually mathematically sensible to constrain the domain of definitions of functions otherwise their definition will require significantly more work and confusion or one will be awash in undefined behavior. 


\subsection{Simply Typed Lambda Terms}


\begin{defn}
A \textit{simply typed $\lambda$-calculus} constructed on a set of \textit{atomic types} $\Sigma$ consists of a set of types defined by the inductive rule
\begin{enumerate}
\item $1$ is a type
\item any atomic type $\alpha \in \Sigma$ is a type
\item if $\alpha, \nu$ are types then $\alpha \times \nu$ is a type
\item if $\alpha, \nu$ are types then $\alpha \to \nu$ is a type
\end{enumerate}
for each type $\sigma$ there is an infinite set of variables $V_\sigma$ of type $\sigma$ write $x : \sigma$ for $x \in V_\sigma$. A \textit{context} or \textit{environment} $\Gamma$ is a finite list of typed variables. We say $M$ is a \textit{term} of type $\alpha$ in $\Gamma$ if the typing judgement $\Gamma \proves M : \alpha$ can be derived through the rules:
\begin{prooftree}
\AxiomC{}
\RightLabel{\scriptsize{UNIT}}
\UnaryInfC{$\Gamma \proves \star : 1$}
\end{prooftree}
\bigskip
\begin{prooftree}
\AxiomC{}
\RightLabel{\scriptsize{VAR}}
\UnaryInfC{$\Gamma, x : \alpha \proves x : \alpha$}
\end{prooftree}
\bigskip
\begin{prooftree}
\AxiomC{$\Gamma, x : \alpha \proves M : \nu$}
\RightLabel{\scriptsize{ABS}}
\UnaryInfC{$\Gamma \proves (\lambda x : \alpha . M) : \alpha \to \nu$}
\end{prooftree}
\bigskip
\begin{prooftree}
\AxiomC{$\Gamma \proves M : \alpha \to \nu$}
\AxiomC{$\Gamma \proves N : \alpha$}
\RightLabel{\scriptsize{APP}}
\BinaryInfC{$\Gamma \proves (M \, N) : \nu$}
\end{prooftree}
\bigskip
\begin{prooftree}
\AxiomC{$\Gamma \proves M : \alpha$}
\AxiomC{$\Gamma \proves N : \alpha$}
\RightLabel{\scriptsize{PAIR}}
\BinaryInfC{$\Gamma \proves \left< M, N \right> : \alpha \times \nu$}
\end{prooftree}
\end{defn}

\begin{theorem}
If $\Gamma \proves M : \alpha$ and $\Gamma \proves M : \nu$ then $\alpha = \nu$.
\end{theorem}

Then we define the main operations on terms: $\alpha$-converstion, $\nu$-reduction, and $\eta$-reduction.

\begin{defn}
Given $(\lambda x : \sigma . M)$ and $y : \sigma$ is a variable not appearing in $M$ then we write $\lambda x : \sigma . M \aconv \lambda y : \sigma . M[x := y]$ are $\alpha$-equivalent or \textit{equivalent via $\alpha$-conversion}. Furthermore, if $\Gamma \proves M : 1$ then $M \aconv \star$.
\end{defn}

\begin{prop}
Typing judgements respect $\alpha$-equivalence:
\[ \Gamma \proves (\lambda x : \sigma . M) : \tau \iff \Gamma \proves (\lambda y : \sigma . M[x := y]) : \tau \]
\end{prop}

\begin{defn}
A $\lambda$-term of the form $(\lambda x.M) \, N$ is called a $\nu$-\textit{redex} and its $\nu$-\textit{reduction} is the corresponding term $M [x := N]$ (where we $\alpha$-convert any variables in $N$ to not clash with variables in $M$). We write,
\[ (\lambda x.M) \, N \bredo M [x := N] \]
If a term $M$ can be converted to $N$ via a finite sequence of $\nu$-reductions we write,
\[ M \bred N \]
Then the equivalence relation generated by this (using zigzags) is called $\nu$-equivalence and writen as $M =_\nu N$. 
\end{defn}

\begin{prop}
Suppose that $\Gamma \proves (\lambda x : \sigma : M) : \sigma \to \tau$ and $\Gamma \proves N : \tau$ then $\Gamma \proves M[x := N] : \tau$ therefore if $\Gamma \proves M : \sigma$ and $M \bred N$ then $\Gamma \proves N : \sigma$ so $\nu$-reduction preserves typing judgements.
\end{prop}

\begin{defn}
A term is \textit{normal} if it contains no $\nu$-redexes. 
\end{defn}

\begin{theorem}
Starting from any term $M$ chosing any sequence of $\nu$-reductions produces a normal form independent of the sequence up to $\alpha$-equivalence.
\end{theorem}

\subsection{The Categorical Semantics}

\begin{defn}
Let $\Lambda$ be a simply-typed $\lambda$-calculus (meaning we introduce symbols of fixed terms and equations) then we form a category $C_\Lambda$ whose objects are the types of $\Lambda$ and whose morphisms 
\[ \Hom{C_\Lambda}{\alpha}{\nu} = \{ M \mid \text{closed terms } \proves M : \alpha \to \nu \} / =_{\alpha \nu \eta} \]
Composition for $N : \alpha \to \nu$ and $M : \nu \to \gamma$ is the equivalence class of the term $(M \, N) : \alpha \to \gamma$. 
\end{defn}

\begin{prop}
$C_\Lambda$ is a Cartesian closed category. 
\end{prop}

\begin{defn}
Let $C$ be a Cartesian closed category. Then there is an associated simply typed $\lambda$-calculus called the \textit{internal language}. Its types are the objects of $C$. For each $\sigma \in C$ we define an infinite list of variables $V_\sigma$ and write $x : \sigma$ if $x \in V_\sigma$. For each morphism $f : \alpha \to \nu$ we introduce a new symbol $f$ of type $\alpha \to \nu$ and we define the reduction of $(g \, f) := g \circ f$.
\end{defn}

\begin{theorem}[Lambeck-Scott]
These functors form an equivalence between $\lambda$-calculi and Cartesian closed categories. 
\end{theorem}

\subsection{Expressible Functions}

\subsection{Models}

\section{Curry-Howard}

The Curry-Howard principle says that \textit{proof theory} and the \textit{theory of computation} are two viewpoints on the same mathematical object -- called \textit{proofs} in one and \textit{programs} in the other.
\bigskip\\
The first incarnation of this correspondence is the Brouwer-Heytin-Kolmogorov interpretation of intuitionistic logic: that proofs are ``procedures'' explicitly that a proof of $A \to B$ is a ``transformation'' or ``procedure'' or -- in modern language -- a program that takes proofs of $A$ to proofs of $B$. This is a transformational step in the developement of the viewpoint of \textit{proofs as mathematical objects}. 

\subsection{Curry-Howard-Lambeck for simply typed $\lambda$-calculus}

Here we follow \chref{https://cs.ioc.ee/~amar/notes/ct2019_lecture5.pdf}{these} notes.

\begin{defn}
The sequence calculus for the fragment of intuitionistic logic with connectives $(\wedge, \to)$ has a set of formulas defined as follows:
\begin{enumerate}
\item propositional variables are formulas
\item if $A$ and $B$ are formulas then $A \wedge B$ and $A \to B$ are formulas.
\end{enumerate}
A \textit{context} $\Gamma$ is a list of formulas. A \textit{judgement} is a setence of the form $\Gamma \proves A$ read $\Gamma$ \textit{proves} $A$ that is inductively built from the following structural rules:
\begin{center}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
\AxiomC{}
\RightLabel{\scriptsize{AX}}
\UnaryInfC{$\Gamma, A \proves A$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$\Gamma, A, A, \Delta \proves B$}
\RightLabel{\scriptsize{CTR}}
\UnaryInfC{$\Gamma, A, \Delta \proves B$}
\end{prooftree}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
\AxiomC{$\Gamma_1, A, B, \Delta \proves C$}
\RightLabel{\scriptsize{EXC}}
\UnaryInfC{$\Gamma_1, B, A, \Delta \proves C$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$\Gamma \proves B$}
\RightLabel{\scriptsize{WKN}}
\UnaryInfC{$\Gamma, A \proves B$}
\end{prooftree}
\end{minipage}
\end{center}
and the following logical rules:
\begin{center}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
\AxiomC{$\Gamma, A, B, \Delta \proves C$}
\RightLabel{\scriptsize{$\wedge_L$}}
\UnaryInfC{$\Gamma, A \wedge B, \Delta \proves C$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$\Gamma, A \proves B$}
\RightLabel{\scriptsize{$\to_R$}}
\UnaryInfC{$\Gamma \proves A \to B$}
\end{prooftree}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
\AxiomC{$\Gamma \proves A$}
\AxiomC{$\Delta \proves B$}
\RightLabel{\scriptsize{$\wedge_R$}}
\BinaryInfC{$\Gamma, \Delta \proves A \wedge B$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$\Gamma \proves A$}
\AxiomC{$B, \Delta \proves C$}
\RightLabel{\scriptsize{$\to_L$}}
\BinaryInfC{$\Gamma, A \to B, \Delta \proves C$}
\end{prooftree}
\end{minipage}
\end{center}
\end{defn}


\begin{example}
Lets show how to prove things like $\Gamma, A \wedge B \proves A$. 
\begin{center}
\begin{prooftree}
\AxiomC{}
\RightLabel{\scriptsize{AX}}
\UnaryInfC{$\Gamma, A \proves A$}
\RightLabel{\scriptsize{WKN}}
\UnaryInfC{$\Gamma, A, B \proves A$}
\RightLabel{\scriptsize{$\wedge_L$}}
\UnaryInfC{$\Gamma, A \wedge B \proves A$}
\end{prooftree}
\end{center}
\end{example}

The advantage of natural deduction systems over Hilbert style systems (like those two-column proofs you had to write in high school) is that the context is allowed to change throughout the deduction. In a Hilbert style system every line must be a theorem of the given global context (i.e. each line must be true) while in natural deduction we are allowed to introduce hypotheticals into the context and deduce sentences that are only true in context. 


\begin{example}
Here is a proof of $(A \to B) \wedge (B \to C) \proves A \to C$.
\begin{center}
\begin{prooftree}
\AxiomC{}
\RightLabel{\scriptsize{AX}}
\UnaryInfC{$A \proves A$}
\AxiomC{}
\RightLabel{\scriptsize{AX}}
\UnaryInfC{$B \proves B$}
\RightLabel{\scriptsize{$\to_L$}}
\BinaryInfC{$A, A \to B \proves B$}
\AxiomC{}
\RightLabel{\scriptsize{AX}}
\UnaryInfC{$B \proves B$}
\AxiomC{}
\RightLabel{\scriptsize{AX}}
\UnaryInfC{$C \proves C$}
\RightLabel{\scriptsize{$\to_L$}}
\BinaryInfC{$B, B \to C \proves B$}
\RightLabel{\scriptsize{$\to_L$}}
\BinaryInfC{$A, A \to B, B \to C \proves C$}
\RightLabel{\scriptsize{$\to_R$}}
\UnaryInfC{$A \to B, B \to C \proves A \to C$}
\RightLabel{\scriptsize{$\wedge_L$}}
\UnaryInfC{$(A \to B) \wedge (B \to C) \proves A \to C$}
\end{prooftree}
\end{center}
\end{example}

\begin{defn}[Cut rule]
Consider the rule
\begin{center}
\begin{prooftree}
\AxiomC{$\Gamma \proves A$}
\AxiomC{$\Delta, A \proves B$}
\RightLabel{\scriptsize{CUT}}
\BinaryInfC{$\Gamma, \Delta \proves B$}
\end{prooftree}
\end{center}
\end{defn}

\begin{theorem}[Cut-elimination]
Any theoem proved using CUT can be proved without it.
\end{theorem}

We think of cut elimination as executing ``computation'' in our logical system. If a proof is a program, then the process of simplifying a proof via cut elimination is analogous to computation via $\nu$-reduction as a ``simplification'' process on $\lambda$-terms.


\begin{defn}
A category $C$ is \textit{Cartesian closed} if it has all finite products and there is a right-adjoint to $(-) \times X$ called \textit{internal Hom} or the \textit{exponential object} denoted as $[X \to (-)]$ or sometimes $(-)^X$.
\end{defn}

\begin{defn}
Let $C$ be a Cartesian closed category. An interpretation of the above logic in $C$ is an assignment of an object $\br{A}$ for each formula $A$ such that,
\[ \br{A \wedge B} := \br{A} \times \br{B} \quad \br{A \to B} := [\br{A} \to \br{B}] \]
Each judgment $\Gamma \proves A$ is interpreted as claiming the existence of a morphism $f : \br{\Gamma} \to \br{A}$ where $\br{\Gamma} := \prod_{B \in \Gamma} \br{B}$ but the construction of the morphism \textit{depends on the proof}. The deduction rules are interpreted as the following operations on morphisms
\begin{center}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
\AxiomC{}
\RightLabel{\scriptsize{AX}}
\UnaryInfC{$\Gamma, A \proves A$}
\end{prooftree}
\begin{center}
$\id_{\br{A}} : \br{A} \to \br{A}$
\end{center}
\begin{prooftree}
\AxiomC{$\Gamma, A, A, \Delta \proves B$}
\RightLabel{\scriptsize{CTR}}
\UnaryInfC{$\Gamma, A, \Delta \proves B$}
\end{prooftree}
\begin{center}
compose with $\Delta : \br{A} \to \br{A} \times \br{A}$
\end{center}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
\AxiomC{$\Gamma, A, B, \Delta \proves C$}
\RightLabel{\scriptsize{EXT}}
\UnaryInfC{$\Gamma, B, A, \Delta \proves C$}
\end{prooftree}
\begin{center}
compose with $\br{A} \times \br{B} \to \br{B} \to \br{A}$
\end{center}
\begin{prooftree}
\AxiomC{$\Gamma \proves B$}
\RightLabel{\scriptsize{WKN}}
\UnaryInfC{$\Gamma, A \proves B$}
\end{prooftree}
\begin{center}
compose with $\pi_1 : \br{\Gamma} \times \br{A} \to \br{\Gamma}$
\end{center}
\end{minipage}
\end{center}
and the following logical rules:
\begin{center}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
\AxiomC{$\Gamma, A, B, \Delta \proves C$}
\RightLabel{\scriptsize{$\wedge_L$}}
\UnaryInfC{$\Gamma, A \wedge B, \Delta \proves C$}
\end{prooftree}
\begin{center}
nothing
\end{center}
\begin{prooftree}
\AxiomC{$\Gamma, A \proves B$}
\RightLabel{\scriptsize{$\to_R$}}
\UnaryInfC{$\Gamma \proves A \to B$}
\end{prooftree}
\begin{center}
adjunction: $\lambda : \Hom{C}{\br{\Gamma} \times \br{A}}{\br{B}} \to \Hom{C}{\br{\Gamma}}{[\br{A} \to \br{B}]}$
\end{center}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
\AxiomC{$\Gamma \proves A$}
\AxiomC{$\Delta \proves B$}
\RightLabel{\scriptsize{$\wedge_R$}}
\BinaryInfC{$\Gamma, \Delta \proves A \wedge B$}
\end{prooftree}
\begin{center}
universal property of $\br{A} \times \br{B}$
\end{center}
\begin{prooftree}
\AxiomC{$\Gamma \proves A$}
\AxiomC{$B, \Delta \proves C$}
\RightLabel{\scriptsize{$\to_L$}}
\BinaryInfC{$\Gamma, A \to B, \Delta \proves C$}
\end{prooftree}
\begin{center}
compose with evaluation map $\ev : \br{A} \times [\br{A} \to \br{B}] \to \br{B}$
\end{center}
\end{minipage}
\end{center}
\end{defn}

\begin{theorem}
The interpretations of proofs modulo cut elimination are well-defined. 
\end{theorem}

\begin{proof}
This is exactly the adjoint relations $\ev \circ (\lambda f \times \id_Y) = f$ for any $f : X \times Y \to Z$ and $\lambda(\ev \circ (h \times \id_Y)) = h$ for any $h : X \to [Y \to Z]$.
\end{proof}

\section{Linear Logic}

\subsection{Differentiation}

We define

\begin{center}
\begin{prooftree}
\AxiomC{$! A \proves B$}
\RightLabel{\scriptsize{DIFF}}
\UnaryInfC{$!A,A \proves B$}
\end{prooftree}
\end{center}
via the composition

\begin{center}
\begin{prooftree}
\AxiomC{$! A \proves B$}
\RightLabel{\scriptsize{COCTR}}
\UnaryInfC{$!A,!A \proves B$}
\RightLabel{\scriptsize{CODER}}
\UnaryInfC{$!A,A \proves B$}
\end{prooftree}
\end{center}


\section{References}

\begin{enumerate}
\item \chref{https://www.youtube.com/watch?v=7cPtCpyBPNI}{}

\item \chref{http://www.cs.nott.ac.uk/~pszgmh/dom4.pdf}{}

\item  \chref{https://en.wikipedia.org/wiki/Kleene_fixed-point_theorem}{klenn fix point}

\item \chref{https://cs.ioc.ee/~amar/notes/ct2019_lecture5.pdf}{amar notes}
\end{enumerate}

\section{Linear Logic}

\newcommand{\oto}{\multimap}

We want a logic which can be interpreted in the category of vectorspaces instead of in the category of sets. The problem is that to interpret standard intutitionistic logic (or simply-typed $\lambda$-calculus) we need a Cartesian closed category. This rules out any Abelian category unless we modify the logic sufficiently. This is what we hope to accomplish with linear logic. There are a number of other interpretations of linear logic,

Interpretations:
\begin{enumerate}
\item in order to have linear (interpretable in an abelian category) semantics we need to give up exponential objects
\item a logic that mirrors quantum mechanics (can be interpreted in the category of Hilbert spaces). One can think of the no cloning theorem as an illustration of the nonlinearity of classical logic and the linearity of quantum logic. Precisiely, there does not exist a \textit{linear} diagonal map $\mathcal{H} \to \mathcal{H} \ot \mathcal{H}$ which would correspond to duplication
\item Linear logic captures reasoning where it is important to treat the hypotheses as resources which are not infinitely expendable. For example, from the sentences
\begin{enumerate}
\item ``if I have five dollars I can buy a coffee''
\item ``if I have five dollars I can buy a sandwich'' 
\end{enumerate}
it seems that the following deduction rule:
\begin{center}
\begin{prooftree}
\AxiomC{$\Gamma \proves A \to B$}
\AxiomC{$\Gamma \proves A \to C$}
\BinaryInfC{$\Gamma \proves A \to B \wedge C$}
\end{prooftree}
\end{center}
would allow us to make the fallacious reasoning
\begin{center}
``if I have five dollars I can buy a coffee and a sandich''
\end{center}
In this case, we need to consider the hypothesis $A =$ ''if I have 5 dollars'' as an expendable resource and it is important to track how many times a hypothesis is used in a proof in order to make valid deductions. Heuristically, we say that such proofs must be ``linear'' in the sense that they only use each hypothesis one so any formulas one might write down can be only first order. 
\end{enumerate}

This is an interesting idea but it seems that such a logic will not be very expressive. For example, 
\begin{enumerate}
\item the Church encoding of the numeral 2
\[ \lambda f . \lambda x . (f (f x)) \] 
is nonlinear in $f$ in an essential way.
\end{enumerate}
If we want ot be able to do anything like arithmetic in this logic, we need to have some mechanism for dealiing with nonlinearity. The essential idea is that of ``exponentiation'' we will allow some hypothesis $A$ to be used arbitrarily many times but only if they are modified by the ''exponential'' operator $!A$. We will not present a fragment of \textit{intuitionistic linear logic}. The language is given by infinitely many propositional variables $x, y, z, \dots$ and formed inductively via two binary connectives $\oto$ and $\ot$ and one unary connective $!$. There is a constant $1$. The set of \textit{formlas} is defined inductively as follows: any varaible or constant is a formula and if $A, B$ are formulas then
\begin{enumerate}
\item $A \oto B$
\item $A \ot B$
\item $!A$ 
\end{enumerate}
are formulas. The reason for the new symbol $\ot$ is that it will be interpreted as tensor product in the linear semantics. The new symbol $\oto$ is just to psychologically distinguish the conditional in linear logic from the material conditional of ordinary intuitionistic logic. In fact, $A \to B$ will turn out to be closer in meaning to $! A \oto B$ than $A \oto B$.

\subsection{Deduction Rules}

\begin{center}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
\AxiomC{}
\RightLabel{\scriptsize{AX}}
\UnaryInfC{$\Gamma, A \proves A$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$\Gamma, !A, !A, \Delta \proves B$}
\RightLabel{\scriptsize{CTR}}
\UnaryInfC{$\Gamma, !A, \Delta \proves B$}
\end{prooftree}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
\AxiomC{$\Gamma, A, B, \Delta \proves C$}
\RightLabel{\scriptsize{EXC}}
\UnaryInfC{$\Gamma, B, A, \Delta \proves C$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$\Gamma \proves B$}
\RightLabel{\scriptsize{WKN}}
\UnaryInfC{$\Gamma, A \proves B$}
\end{prooftree}
\end{minipage}
\end{center}
Notice that contraction is only allowed for exponentiated objects. This is because linear proofs using a standard hypothesis twice cannot automatically be reduced to using the hypothesis once.
There are also the following logical rules:
\begin{center}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
\AxiomC{$\Gamma, A, B, \Delta \proves C$}
\RightLabel{\scriptsize{$\otimes_L$}}
\UnaryInfC{$\Gamma, A \otimes B, \Delta \proves C$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$\Gamma, A \proves B$}
\RightLabel{\scriptsize{$\oto_R$}}
\UnaryInfC{$\Gamma \proves A \oto B$}
\end{prooftree}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
\AxiomC{$\Gamma \proves A$}
\AxiomC{$\Delta \proves B$}
\RightLabel{\scriptsize{$\ot_R$}}
\BinaryInfC{$\Gamma, \Delta \proves A \ot B$}
\end{prooftree}
\begin{prooftree}
\AxiomC{$\Gamma \proves A$}
\AxiomC{$\Delta, B \proves C$}
\RightLabel{\scriptsize{$\oto_L$}}
\BinaryInfC{$\Gamma, \Delta, A \oto B \proves C$}
\end{prooftree}
\end{minipage}
\end{center}
there are then the logical rules relating to exponentiation
\begin{center}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
\AxiomC{$! \Gamma \proves A$}
\RightLabel{\scriptsize{PRO}}
\UnaryInfC{$! \Gamma \proves !A$}
\end{prooftree}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
\AxiomC{$\Gamma A, \Delta \proves B$}
\RightLabel{\scriptsize{DER}}
\UnaryInfC{$\Gamma, !A, \Delta \proves B$}
\end{prooftree}
\end{minipage}
\end{center}
there are also some obvious rules involving $1$
\begin{center}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
\AxiomC{$\Gamma, \Delta \proves A$}
\RightLabel{\scriptsize{1-L}}
\UnaryInfC{$\Gamma, 1, \Delta \proves A$}
\end{prooftree}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
\AxiomC{}
\RightLabel{\scriptsize{1-R}}
\UnaryInfC{$\proves 1$}
\end{prooftree}
\end{minipage}
\end{center}

\begin{example}
Note that $! A \proves A \ot A$ but $A \not\proves A \ot A$. Indeed, consider the following proof
\begin{center}
\begin{prooftree}
\AxiomC{}
\RightLabel{\scriptsize{AX}}
\UnaryInfC{$A \proves A$}
\AxiomC{}
\RightLabel{\scriptsize{AX}}
\UnaryInfC{$A \proves A$}
\RightLabel{\scriptsize{$\ot_R$}}
\BinaryInfC{$A, A \proves A \ot A$}
\end{prooftree}
\end{center}
but notice we cannot apply contraction to get $A \proves A \ot A$. Indeed this corresponds to the fact that there is no linear diagonal map $V \to V \to V$ but given $\alpha \in V$ and $\beta \in V$ there is $\alpha \ot \beta \in V \ot V$. However, if we derelict $A$ then we can make the proof go through
\begin{center}
\begin{prooftree}
\AxiomC{}
\RightLabel{\scriptsize{AX}}
\UnaryInfC{$A \proves A$}
\RightLabel{\scriptsize{DER}}
\UnaryInfC{$!A \proves A$}
\AxiomC{}
\RightLabel{\scriptsize{AX}}
\UnaryInfC{$A \proves A$}
\RightLabel{\scriptsize{DER}}
\UnaryInfC{$!A \proves A$}
\RightLabel{\scriptsize{$\ot_R$}}
\BinaryInfC{$!A, !A \proves A \ot A$}
\RightLabel{\scriptsize{CTR}}
\UnaryInfC{$!A \proves A \ot A$}
\end{prooftree}
\end{center}
\end{example}

\begin{example}
For any fomula $A$ let 
\[ \bf{int}_A := \, !(A \oto A) \oto (A \oto A) \]
by the type of integers valued in $A$. This exactly corresponds to the Church encoding of the integers as functions that take a function $f : A \to A$ and return a function $A \to A$ which is the $n^{\text{th}}$-iterate of $f$. However, note that in linear logic we need to exponentiate the hypothesis $(A \to A)$ in order to iterate $f$. Recall, that under the Curry-Howard correspondence, functions of type $\bf{int}_A$ correspond to proofs $\proves \bf{int}_A$. The object $\ul{1}_A$ is the following proof of $\bf{int}_A$ 
\begin{center}
\begin{prooftree}
\AxiomC{}
\RightLabel{\scriptsize{AX}}
\UnaryInfC{$A \oto A \proves A \oto A$}
\RightLabel{\scriptsize{DER}}
\UnaryInfC{$!(A \oto A) \proves A \oto A$}
\RightLabel{\scriptsize{$\oto_R$}}
\UnaryInfC{$\proves !(A \oto A) \oto (A \oto A)$}
\end{prooftree}
\end{center}
whereas $\ul{0}_A$ is the following proof
\begin{center}
\begin{prooftree}
\AxiomC{}
\RightLabel{\scriptsize{AX}}
\UnaryInfC{$A \proves A$}
\RightLabel{\scriptsize{$\oto_R$}}
\UnaryInfC{$\proves A \oto A$}
\RightLabel{\scriptsize{WKN}}
\UnaryInfC{$!(A \oto A) \proves A \oto A$}
\RightLabel{\scriptsize{$\oto_R$}}
\UnaryInfC{$\proves !(A \oto A) \oto (A \oto A)$}
\end{prooftree}
\end{center}
Finally, the proof $\ul{2}_A$ is a bit more complicated,
\begin{center}
\begin{prooftree}
\AxiomC{}
\UnaryInfC{$A \proves A$}
\AxiomC{}
\UnaryInfC{$A \proves A$}
\AxiomC{}
\UnaryInfC{$A \proves A$}
\RightLabel{$\oto_L$}
\BinaryInfC{$A, A \oto A \proves A$}
\RightLabel{$\oto_L$}
\BinaryInfC{$A, A \oto A, A \oto A \proves A$}
\RightLabel{$\oto_R$}
\UnaryInfC{$A \oto A, A \oto A \proves A \oto A$}
\RightLabel{\scriptsize{DER}}
\UnaryInfC{$!(A \oto A), A \oto A \proves A \oto A$}
\RightLabel{\scriptsize{DER}}
\UnaryInfC{$!(A \oto A), !(A \oto A) \proves A \oto A$}
\RightLabel{\scriptsize{CTR}}
\UnaryInfC{$!(A \oto A) \proves A \oto A$}
\RightLabel{$\oto_R$}
\UnaryInfC{$\proves !(A \oto A) \oto (A \oto A)$}
\end{prooftree}
\end{center}
To understand why this corresponds to the number $2$ we need to understand cut elimination.
\end{example}

\begin{defn}[Cut rule]
Consider the rule
\begin{center}
\begin{prooftree}
\AxiomC{$\Gamma \proves A$}
\AxiomC{$\Delta, A \proves B$}
\RightLabel{\scriptsize{CUT}}
\BinaryInfC{$\Gamma, \Delta \proves B$}
\end{prooftree}
\end{center}
\end{defn}

\begin{theorem}[Cut-elimination]
Any theoem proved using CUT can be proved without it.
\end{theorem}

\newcommand{\cuteq}{=_{\text{cut}}}
\newcommand{\cutred}{\red_{\text{cut}}}
\newcommand{\cP}{\mathcal{P}}

Cut-elimination really says something much more interesting about the structure of the proofs themselves. The real content is in giving a sequence of elementary cut-elimination rules on profs allowing cuts and prove that after a finite number of steps we end up at a cut-free proof. These elementary reductions are intended to mirror $\nu$-reduction via the Curry-Howard correspondence or alternatively they are the fundamental adjunction relations in the Categorical semantics we will now explore. 

\begin{defn}
Let $\Gamma \proves A$ be a judgement in linear logic and $\cP$ the set of proofs. There is a birary relation $\rho \cutred \pi$ if $\pi$ is an immediate reduction of $\rho$ by an elementary cut-elimination rule. We refer to the equivalence relation $\cuteq$ generated by $\cutred$ as \textit{cut-equivalence} and the steps \textit{cut-elimination transformations}.
\end{defn}

\begin{example}
Let $\pi$ be any proof of $A \proves A$ then the proof of $A \oto A$ denoted $\ul{2}_A \mid \text{prom}(\pi)$
\begin{center}
\begin{prooftree}
\AxiomC{}
\RightLabel{$\pi$}
\UnaryInfC{$A \proves A$}
\RightLabel{$\oto_R$}
\UnaryInfC{$\proves A \oto A$}
\RightLabel{\scriptsize{PRO}}
\UnaryInfC{$\proves !(A \oto A)$}
\AxiomC{}
\RightLabel{$\ul{2}_A$}
\UnaryInfC{$!(A \oto A) \proves A \oto A$}
\RightLabel{\scriptsize{CUT}}
\BinaryInfC{$\proves A \oto A$}
\end{prooftree}
\end{center}
is cut equivalent to the proof $\pi \mid \pi$
\begin{center}
\begin{prooftree}
\AxiomC{}
\RightLabel{$\pi$}
\UnaryInfC{$A \proves A$}
\AxiomC{}
\RightLabel{$\pi$}
\UnaryInfC{$A \proves A$}
\RightLabel{\scriptsize{CUT}}
\BinaryInfC{$A \proves A$}
\RightLabel{\scriptsize{$\oto_R$}}
\UnaryInfC{$\proves A \oto A$}
\end{prooftree}
\end{center}
which represents the composition of $\pi$ with itself. 
\end{example}

\subsection{Linear Semantics}

Let $k$ be a field {\color{red} why in the notes algebraically closed and of characteristic zero}. 

\begin{defn}
A \textit{denotation} is an assignment $A \mapsto \br{A}$ from sentences to $k$-vectorspaces defined inductively
\begin{enumerate}
\item the propositional variabesl are assigned to {\color{red} why in the notes finite dimensional?} vectorspaces
\item $\br{1} = k$
\item $\br{A \ot B} = \br{A} \ot \br{B}$
\item $\br{A \oto B} = [\br{A} \to \br{B}]$
\item $\br{!A} = ! \br{A}$ where $!V$ is the cofree coalgebra.
\end{enumerate}
Furthermore, there are denotational rules for proofs as morphisms parallel to our previous construction. We will only write down the new rules corresponding to the exponential
\begin{center}
\begin{minipage}{0.45\textwidth}
\begin{prooftree}
\AxiomC{$\Gamma \proves A$}
\AxiomC{$\Delta \proves B$}
\RightLabel{\scriptsize{$\wedge_R$}}
\BinaryInfC{$\Gamma, \Delta \proves A \wedge B$}
\end{prooftree}
\begin{center}
universal property of $\br{A} \times \br{B}$
\end{center}
\begin{prooftree}
\AxiomC{$\Gamma \proves A$}
\AxiomC{$B, \Delta \proves C$}
\RightLabel{\scriptsize{$\to_L$}}
\BinaryInfC{$\Gamma, A \to B, \Delta \proves C$}
\end{prooftree}
\begin{center}
compose with evaluation map $\ev : \br{A} \times [\br{A} \to \br{B}] \to \br{B}$
\end{center}
\end{minipage}
\end{center}
\end{defn}

\newcommand{\Sym}[1]{\mathrm{Sym}\left( #1 \right)}
\newcommand{\ket}[1]{\left| #1 \right>}

\begin{defn}
The \textit{cofree coalgebra} $!V$ is the universal coalgebra equipped with a map $\d : \, ! V \to V$ universal in the sense that given any linear map $\phi : C \to V$ from a coalgebra there is a unique morphism of coalgebras $\Phi : C \to !V$ such that
\begin{center}
\begin{tikzcd}
! V \arrow[r, "\d"] & V 
\\
C \arrow[ru, "\phi"] \arrow[u, "\Phi", dashed]
\end{tikzcd}
\end{center}
If $V$ is a finite-dimensional vector space then
\[ !V = \bigoplus_{v \in V} \Sym{V} \ket{\empty}_v \]
we write $\ket{v_1, \dots, v_n}_v := v_1, \dots, v_n \ket{\empty}_v$ then the comultiplication structure is
\[ ! V \to ! V \ot ! V \quad {v_1, \dots, v_n}_v \mapsto \sum_{I \subset \{1, \dots, n \}} \ket{v_I}_v \ot \ket{v_{I^c}}_v \]
and there is a canonical linear map
\[ \d : \, !V \to V \quad \ket{\empty}_v \mapsto v \]
and all other $v_1 \ot \cdots \ot v_n \ket{\empty}_v \mapsto 0$. 
\end{defn}

\begin{rmk}
Notice that there is a nonlinear map $V \to ! V$ given by $v \mapsto \ket{\empty}_v$ this is important for the interpretation of morphisms arising from proofs involving dereliction.
\end{rmk}

\begin{example}
Let $A$ be a formula and $A = \br{A}$. The denotation of the proof $\ul{2}_A$ of $\proves \bf{int}_A$ is the morphism
\[ \br{\ul{2}_A} : k \to \Hom{k}{! \End[k]{V}}{\End[k]{V}} \]
which is the linear map
\[ ! \End[k]{V} \xrightarrow{\Delta} ! \End[k]{V} \ot \, ! \End[k]{V} \xrightarrow{\d \ot \d} \End[k]{V}^{\ot 2} \xrightarrow{- \circ -} \End[k]{V} \]
Therefore $\br{\ul{2}_A}$ is the map
\[ \ket{\empty}_\alpha \mapsto \ket{\empty}_\alpha \ot \ket{\empty}_\alpha \mapsto \alpha \ot \alpha \mapsto \alpha \circ \alpha \]
\end{example}

\subsection{Derivatives}

\newcommand{\Spec}[1]{\mathrm{Spec}\left( #1 \right)}
\newcommand{\T}{\mathcal{T}}

Let's investigate a bit more what the map $\br{\ul{2}_A}$ does. Indeed, consider its action on elements of the form $\ket{\nu}_\alpha$ where $\nu, \alpha \in \End[k]{V}$. Indeed,
\[ \ket{\nu}_\alpha \mapsto \ket{\nu}_\alpha \ot \ket{\empty}_\alpha + \ket{\empty}_\alpha \ot \ket{\nu}_\alpha \mapsto \nu \ot \alpha + \alpha \ot \nu \mapsto \nu \circ \alpha + \alpha \circ \nu = \{ \nu, \alpha \} \] 
which looks like the derivative of the squaring map 
\[ \alpha \mapsto \alpha \circ \alpha \]
at $\alpha$ in the direction $\nu$. This interpretation suggests a definition of the derivative in general. 
\bigskip\\
For each proof $\proves \alpha : A$ there is a vector $\br{\alpha} \in \br{A}$ (the corresponding morphism $1 \to \br{A}$) and hence a group-like element $\ket{\empty}_{\br{\alpha}} \in ! \br{A}$. Likewise, givien a proof $\psi : \, !A \proves B$ the denotation is a linear map
\[ \br{\psi} : \, ! \br{A} \to \br{B} \]
It is the behavior of the denotation of $\br{\psi}$ on these group-like elements which recovers the input-output behavior of $\psi$ since
\[ \br{\psi} \ket{\empty}_{\br{\alpha}} = \br{\psi(\alpha)} \]
where $\psi(\alpha) : B$ is the proof of $B$ given by applying cut-elimination to $\psi \mid \text{prom}(\alpha)$. However there is more information in the linear map $\br{\psi}$ than the input-output behavior of $\psi$. For example, we can lift $\br{\psi}$ to a coalgebra morphism
\[ ! \br{\psi} : ! \br{A} \to ! \br{B} \]
and given proofs $\alpha, \beta : A$ there is an associated primitive element $\ket{\br{\beta}}_{\br{\alpha}} \in ! \br{A}$ and in fact if we take the limit formally (the output will be polynomial in $h$)
\[ \br{\psi} \ket{\br{\beta}}_{\br{\alpha}} = \lim_{h \to 0} \frac{\br{\psi} \ket{\empty}_{\br{\alpha} + h \br{\beta}} - \br{\psi} \ket{\empty}_{\br{\alpha}}}{h} \]
There is an algebraic interpretation of this derivative. Algebraic geometry teaches us that if we consider a finite-dimensional vector space as a scheme $\ul{V} = \Spec{\Sym{V^\vee}}$ then a tangent vector $\xi$ of this scheme at a point $v \in V$ is a map
\[ \Spec{k[\epsilon]} \to \ul{V} \]
such that $\Spec{k} \to \Spec{k[\epsilon]}$ maps to $v$. Then pairs $(v, \xi)$ correspond to 
\[ \Hom{k\text{-alg}}{\Sym{V^\vee}}{k[\epsilon]} = \Hom{k}{V^\vee}{k[\epsilon]} = \Hom{k}{(k[\epsilon])^*}{V} = \Hom{k\text{-coalg}}{(k[\epsilon])^*}{!V} \]
where $k[\epsilon]$ is a Hopf algebra and thus its dual, we call $\T$ with basis $1, \varepsilon = \epsilon^*$, is also a coalgebra. 

\begin{lemma}
Coalgebra morphisms $\T \to ! V$ are always of the form $\theta(1) = \ket{\empty}_\alpha$ and $\theta(\varepsilon) = \ket{u}_v$ for some $v,u \in V$. 
\end{lemma}

\begin{proof}
Indeed $\Delta(1) = 1 \ot 1$ so $\theta(1)$ is grouplike and hence is of the form $\ket{\empty}_v$. Furthermore $\Delta(\varepsilon) = \varepsilon \ot 1 + 1 \ot \varepsilon$ and therefore $\theta(\epsilon)$ must be of the form $\ket{u}_{v}$ since these are the only elements satisfing $\Delta(\ket{u}_v) = \ket{u}_v \ot \ket{\empty}_v + \ket{\empty}_v \ot \ket{u}_v$.
\end{proof}


\begin{defn}
Given a proof $\proves \alpha : A$ a \textit{tangent vector} at $\pi$ is a morphism of coalgebras $\theta : \T \to ! \br{A}$ with the property that $\theta(1) = \ket{\empty}_{\br{\pi}}$ meaning that the diagram
\begin{center}
\begin{tikzcd}
k \arrow[d] \arrow[r, "\br{\pi}"] & \br{A} 
\\
\T \arrow[r, "\theta"] & ! \br{A} \arrow[u, "\d"] 
\end{tikzcd}
\end{center}
Notice that there is an isomorphism between $\br{A}$ and the tangent space given by sending $Q \in \br{A}$ to the coalgebra morphism $\theta : \T \to ! \br{A}$ such that
\[ \theta(1) = \ket{\empty}_{\br{\pi}} \quad \quad \theta(\epsilon) = \ket{Q}_{\br{\pi}} \]
\end{defn}

Note that the denotation of a program not only maps inputs to outputs but also tangent vectors to tangent vectors. Indeed, given $\br{\psi} : \, ! \br{A} \to \br{B}$ we lift to $! \br{\psi} : \, ! \br{A} \to ! \br{B}$ then we get a map
\[ \T \xrightarrow{\theta} ! \br{A} \xrightarrow{! \br{\psi}} ! \br{B} \]
which is a tangent vector at the proof $\psi(\alpha) := \rho \mid \text{prom}(\pi)$. This is equivalent to the following definition.

\begin{defn}
Given $\proves \pi : \, ! A \oto B$ and proves $\proves \alpha, \beta : A$ then we interpret $\br{\alpha}, \br{\beta} \in \br{A}$ then we consider
\begin{center}
\begin{tikzcd}[column sep = huge, row sep = huge]
\br{!A} \arrow[r, "\br{\pi}"] & \br{B} 
\\
\T \arrow[u, "\br{\alpha} \oplus \br{\beta}"] \arrow[r, "\br{\pi(\alpha)} \oplus \br{\partial_\beta \alpha}"', dashed] & \br{!B} \arrow[u]
\end{tikzcd}
\end{center}
where the right map exists by the universal property of $\br{! B} = ! \br{B}$ and then $\epsilon \mapsto \br{\partial_\beta \alpha}$.
\end{defn}

\subsubsection{Encoding the product rule as a cut-elimination rule}

We have shown how to define the derivative in terms of the linear semantics. It turns our there is a purely syntactic definition of the derivative in linear logic. It consists of adding codereliction and cocontraction rules along with corresponding cut-elimination rules. Then the derivative is
\begin{center}
\begin{minipage}{0.3\textwidth}
\begin{prooftree}
\AxiomC{}
\RightLabel{$\pi$}
\UnaryInfC{$!A \proves B$}
\RightLabel{\scriptsize{DIFF}}
\UnaryInfC{$!A, A \proves A$}
\end{prooftree}
\end{minipage}
\begin{minipage}{0.3\textwidth}
\text{is defined to be}
\end{minipage}
\begin{minipage}{0.3\textwidth}
\begin{prooftree}
\AxiomC{}
\RightLabel{$\pi$}
\UnaryInfC{$!A \proves B$}
\RightLabel{\scriptsize{COCTR}}
\UnaryInfC{$!A, !A \proves A$}
\RightLabel{\scriptsize{CODER}}
\UnaryInfC{$!A, A \proves B$}
\end{prooftree}
\end{minipage}
\end{center}
Then the product rule is encoded in the following cut-elimination rule
\begin{center}
\begin{prooftree}
\AxiomC{}
\RightLabel{$\partial_A$}
\UnaryInfC{$!A,A \proves B$}
\AxiomC{}
\RightLabel{$\pi$}
\UnaryInfC{$!A, !A \proves B$}
\RightLabel{\scriptsize{CTR}}
\UnaryInfC{$!A \proves B$}
\BinaryInfC{$!A, A \proves B$}
\end{prooftree}
\end{center}
reduces via cut-elimination to
\begin{center}
\begin{minipage}{0.3\textwidth}
\begin{prooftree}
\AxiomC{}
\RightLabel{$\partial_A$}
\UnaryInfC{$!A,A \proves !A$}
\AxiomC{}
\RightLabel{$\pi$}
\UnaryInfC{$!A, !A \proves B$}
\RightLabel{\scriptsize{CUT}}
\BinaryInfC{$!A, !A, A \proves B$}
\RightLabel{\scriptsize{CTR}}
\UnaryInfC{$!A, A \proves B$}
\end{prooftree}
\end{minipage}
\begin{minipage}{0.3\textwidth}
\quad \quad \quad \quad \quad \quad \quad  $+$
\end{minipage}
\begin{minipage}{0.3\textwidth}
\begin{prooftree}
\AxiomC{}
\RightLabel{$\partial_A$}
\UnaryInfC{$!A,A \proves !A$}
\AxiomC{}
\RightLabel{$\pi$}
\UnaryInfC{$!A, !A \proves B$}
\RightLabel{\scriptsize{EXC}}
\UnaryInfC{$!A, !A \proves B$}
\RightLabel{\scriptsize{CUT}}
\BinaryInfC{$!A, !A, A \proves B$}
\RightLabel{\scriptsize{CTR}}
\UnaryInfC{$!A, A \proves B$}
\end{prooftree}
\end{minipage}
\end{center}

\subsection{Example}

\subsection{Probabilistic Interpretation}

We are going to think of $! V$ as the space of probability distributions on $V$. Think of $\ket{\empty}_v$ as the detla distribution at $v \in V$ and $\ket{v_1, \dots, v_n}_v$ as the weak derivatives
\[ \partial_{v_1} \cdots \partial_{v_n} v \]
meaning they integrate against a function $f : V \to k$ to give 
\[ (-1)^n \partial_{v_1} \cdots \partial_{v_n} f(0) \]
We think of the lifting 
\[ ! \br{\psi} : ! \br{A} \to ! \br{B} \]
as the natural extension of a measurable map to the space of distributions via pushforward measure. 
\bigskip\\
Therefore, if we understand what the underlying spaces $\br{A}$ and $\br{B}$ represent for a given program, we can understand the derivative as an action on probability distributions (with finite support) over those spaces. Indeed we see that,
\[ \partial_{\br{\beta}} \br{\psi}(\br{\alpha}) = \br{\psi}_* \nabla_{\br{\beta}} \delta_{\br{\alpha}} \]
\bigskip\\
Given $v_1, \dots, v_n \in V$ and real numbers $0 \le a_i \le 1$ there are two encodings of their distributions
\[ \sum_i a_i \ket{\empty}_{v_i} \quad \quad \ket{\empty}_{\sum a_i v_i} \]
the former we call the \textit{standard encoding} and the later the \textit{naive encoding}. 

\end{document}
